{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ToyRL","text":""},{"location":"#documentation","title":"Documentation","text":"<p>https://ai-glimpse.github.io/toyrl</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install toyrl\n</code></pre>"},{"location":"#algorithms","title":"Algorithms","text":"<ul> <li> REINFORCE</li> <li> SARSA</li> <li> DQN &amp; Double DQN</li> <li> A2C</li> <li> PPO</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>kengz/SLM-Lab: Our implementations are inspired by the book \"Foundations of Deep Reinforcement Learning\" and the implementation of SLM-Lab.</li> <li>vwxyzjn/cleanrl: The main reference for the implementation of the PPO implementation.</li> </ul>"},{"location":"algorithms/a2c/","title":"A2C","text":""},{"location":"algorithms/a2c/#toyrl.a2c.default_config","title":"toyrl.a2c.default_config  <code>module-attribute</code>","text":"<pre><code>default_config = A2CConfig(env_name='CartPole-v1', render_mode=None, solved_threshold=475.0, num_episodes=100000, learning_rate=0.002, log_wandb=True)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.trainer","title":"toyrl.a2c.trainer  <code>module-attribute</code>","text":"<pre><code>trainer = A2CTrainer(default_config)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig","title":"toyrl.a2c.A2CConfig  <code>dataclass</code>","text":"<pre><code>A2CConfig(env_name: str = 'CartPole-v1', render_mode: str | None = None, solved_threshold: float = 475.0, gamma: float = 0.999, lambda_: float = 0.98, value_loss_coef: float = 0.5, policy_loss_coef: float = 0.5, entropy_coef: float = 0.01, num_episodes: int = 500, learning_rate: float = 0.01, eval_episodes: int = 10, eval_interval: int = 100, log_wandb: bool = False)\n</code></pre> <p>Configuration for A2C algorithm.</p>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.env_name","title":"env_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_name: str = 'CartPole-v1'\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.render_mode","title":"render_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>render_mode: str | None = None\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.solved_threshold","title":"solved_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>solved_threshold: float = 475.0\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.999\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.lambda_","title":"lambda_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lambda_: float = 0.98\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.value_loss_coef","title":"value_loss_coef  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>value_loss_coef: float = 0.5\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.policy_loss_coef","title":"policy_loss_coef  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>policy_loss_coef: float = 0.5\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.entropy_coef","title":"entropy_coef  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entropy_coef: float = 0.01\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.num_episodes","title":"num_episodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_episodes: int = 500\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.01\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.eval_episodes","title":"eval_episodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval_episodes: int = 10\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.eval_interval","title":"eval_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eval_interval: int = 100\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CConfig.log_wandb","title":"log_wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_wandb: bool = False\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet","title":"toyrl.a2c.ActorCriticNet","text":"<pre><code>ActorCriticNet(env_dim: int, action_num: int)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>toyrl/a2c.py</code> <pre><code>def __init__(self, env_dim: int, action_num: int) -&gt; None:\n    super().__init__()\n    self.env_dim = env_dim\n    self.action_num = action_num\n    self.shared_layers = nn.Sequential(\n        nn.Linear(env_dim, 64),\n        nn.ReLU(),\n    )\n    self.policy_head = nn.Linear(64, action_num)\n    self.value_head = nn.Linear(64, 1)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.env_dim","title":"env_dim  <code>instance-attribute</code>","text":"<pre><code>env_dim = env_dim\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.action_num","title":"action_num  <code>instance-attribute</code>","text":"<pre><code>action_num = action_num\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.shared_layers","title":"shared_layers  <code>instance-attribute</code>","text":"<pre><code>shared_layers = Sequential(Linear(env_dim, 64), ReLU())\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.policy_head","title":"policy_head  <code>instance-attribute</code>","text":"<pre><code>policy_head = Linear(64, action_num)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.value_head","title":"value_head  <code>instance-attribute</code>","text":"<pre><code>value_head = Linear(64, 1)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ActorCriticNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    x = self.shared_layers(x)\n    policy_action_logits = self.policy_head(x)\n    v_value = self.value_head(x)\n    return policy_action_logits, v_value\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience","title":"toyrl.a2c.Experience  <code>dataclass</code>","text":"<pre><code>Experience(observation: Any, action: Any, reward: float, next_observation: Any, terminated: bool, truncated: bool)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.observation","title":"observation  <code>instance-attribute</code>","text":"<pre><code>observation: Any\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Any\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.next_observation","title":"next_observation  <code>instance-attribute</code>","text":"<pre><code>next_observation: Any\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.terminated","title":"terminated  <code>instance-attribute</code>","text":"<pre><code>terminated: bool\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Experience.truncated","title":"truncated  <code>instance-attribute</code>","text":"<pre><code>truncated: bool\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer","title":"toyrl.a2c.ReplayBuffer  <code>dataclass</code>","text":"<pre><code>ReplayBuffer(buffer: list[Experience] = list())\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: list[Experience] = field(default_factory=list)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.buffer)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self.buffer.append(experience)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def reset(self) -&gt; None:\n    self.buffer = []\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.sample","title":"sample","text":"<pre><code>sample() -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def sample(self) -&gt; list[Experience]:\n    return self.buffer\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.ReplayBuffer.total_reward","title":"total_reward","text":"<pre><code>total_reward() -&gt; float\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def total_reward(self) -&gt; float:\n    return sum(experience.reward for experience in self.buffer)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent","title":"toyrl.a2c.Agent","text":"<pre><code>Agent(net: Module, optimizer: Optimizer)\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def __init__(self, net: nn.Module, optimizer: torch.optim.Optimizer) -&gt; None:\n    self._net = net\n    self._optimizer = optimizer\n    self._replay_buffer = ReplayBuffer()\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent.onpolicy_reset","title":"onpolicy_reset","text":"<pre><code>onpolicy_reset() -&gt; None\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def onpolicy_reset(self) -&gt; None:\n    self._replay_buffer.reset()\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self._replay_buffer.add_experience(experience)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent.get_buffer_total_reward","title":"get_buffer_total_reward","text":"<pre><code>get_buffer_total_reward() -&gt; float\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def get_buffer_total_reward(self) -&gt; float:\n    return self._replay_buffer.total_reward()\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent.act","title":"act","text":"<pre><code>act(observation: ndarray, eval: bool = False) -&gt; int\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def act(self, observation: np.ndarray, eval: bool = False) -&gt; int:\n    x = torch.from_numpy(observation.astype(np.float32))\n    with torch.no_grad():\n        action_logits, _ = self._net(x)\n    next_action_dist = torch.distributions.Categorical(logits=action_logits)\n    action = next_action_dist.sample()\n    if eval:\n        action = next_action_dist.probs.argmax(dim=-1)\n    return action.item()\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.Agent.net_update","title":"net_update","text":"<pre><code>net_update(gamma: float, lambda_: float, value_loss_coef: float, policy_loss_coef: float, entropy_coef: float) -&gt; tuple[float, float, float]\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def net_update(\n    self, gamma: float, lambda_: float, value_loss_coef: float, policy_loss_coef: float, entropy_coef: float\n) -&gt; tuple[float, float, float]:\n    experiences = self._replay_buffer.sample()\n\n    observations = torch.tensor([exp.observation for exp in experiences])\n    actions = torch.tensor([exp.action for exp in experiences])\n    rewards = torch.tensor([exp.reward for exp in experiences]).unsqueeze(1)\n    next_observations = torch.tensor([exp.next_observation for exp in experiences])\n    terminateds = torch.tensor([exp.terminated for exp in experiences], dtype=torch.float32).unsqueeze(1)\n\n    # calculate predicted V-values\n    policy_action_logits, v_values = self._net(observations)\n    # n-step return\n    v_targets = torch.zeros_like(rewards)\n    for t in reversed(range(len(experiences) - 1)):\n        v_targets[t] = rewards[t] + gamma * v_targets[t + 1] * (1 - terminateds[t])\n    # calculate value loss\n    value_loss = nn.functional.mse_loss(v_values, v_targets)\n\n    # calculate advantages by GAE\n    with torch.no_grad():\n        _, v_values_next = self._net(next_observations)\n    deltas = rewards + gamma * v_values_next * (1 - terminateds) - v_values\n    advantages = deltas.clone()\n    for t in reversed(range(len(experiences) - 1)):\n        advantages[t] = deltas[t] + gamma * lambda_ * advantages[t + 1] * (1 - terminateds[t])\n    advantages = advantages / (advantages.std() + 1e-8)\n    advantages = advantages.detach()\n\n    action_dist = torch.distributions.Categorical(logits=policy_action_logits)\n    action_entropy = action_dist.entropy().mean()\n    action_log_probs = action_dist.log_prob(actions)\n    # calculate policy loss\n    policy_loss = -action_log_probs * advantages\n    policy_loss = torch.mean(policy_loss)\n\n    loss = value_loss * value_loss_coef + policy_loss * policy_loss_coef - entropy_coef * action_entropy\n\n    # update\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    return loss.item(), action_entropy.item(), advantages.mean().item()\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer","title":"toyrl.a2c.A2CTrainer","text":"<pre><code>A2CTrainer(config: A2CConfig)\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def __init__(self, config: A2CConfig) -&gt; None:\n    self.config = config\n    self.env = gym.make(config.env_name, render_mode=config.render_mode)\n    env_dim = self.env.observation_space.shape[0]  # type: ignore[index]\n    action_num = self.env.action_space.n  # type: ignore[attr-defined]\n    net = ActorCriticNet(env_dim=env_dim, action_num=action_num)\n    optimizer = optim.Adam(net.parameters(), lr=config.learning_rate)\n    self.agent = Agent(net=net, optimizer=optimizer)\n\n    self.num_episodes = config.num_episodes\n    self.gamma = config.gamma\n    self.lambda_ = config.lambda_\n    self.value_loss_coef = config.value_loss_coef\n    self.policy_loss_coef = config.policy_loss_coef\n    self.entropy_coef = config.entropy_coef\n    self.solved_threshold = config.solved_threshold\n    if config.log_wandb:\n        wandb.init(\n            # set the wandb project where this run will be logged\n            project=\"A2C\",\n            name=f\"[{config.env_name}]lr={config.learning_rate}\",\n            # track hyperparameters and run metadata\n            config=asdict(config),\n        )\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = make(env_name, render_mode=render_mode)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = Agent(net=net, optimizer=optimizer)\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.num_episodes","title":"num_episodes  <code>instance-attribute</code>","text":"<pre><code>num_episodes = num_episodes\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.lambda_","title":"lambda_  <code>instance-attribute</code>","text":"<pre><code>lambda_ = lambda_\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.value_loss_coef","title":"value_loss_coef  <code>instance-attribute</code>","text":"<pre><code>value_loss_coef = value_loss_coef\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.policy_loss_coef","title":"policy_loss_coef  <code>instance-attribute</code>","text":"<pre><code>policy_loss_coef = policy_loss_coef\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.entropy_coef","title":"entropy_coef  <code>instance-attribute</code>","text":"<pre><code>entropy_coef = entropy_coef\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.solved_threshold","title":"solved_threshold  <code>instance-attribute</code>","text":"<pre><code>solved_threshold = solved_threshold\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.train","title":"train","text":"<pre><code>train() -&gt; None\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def train(self) -&gt; None:\n    for i, episode in enumerate(range(self.num_episodes)):\n        observation, _ = self.env.reset()\n        terminated, truncated = False, False\n        while not (terminated or truncated):\n            action = self.agent.act(observation)\n            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n            experience = Experience(\n                observation=observation,\n                action=action,\n                reward=float(reward),\n                terminated=terminated,\n                truncated=truncated,\n                next_observation=next_observation,\n            )\n            self.agent.add_experience(experience)\n            observation = next_observation\n            if self.config.render_mode is not None:\n                self.env.render()\n        loss, action_entropy, advantages_mean = self.agent.net_update(\n            gamma=self.gamma,\n            lambda_=self.lambda_,\n            value_loss_coef=self.value_loss_coef,\n            policy_loss_coef=self.policy_loss_coef,\n            entropy_coef=self.entropy_coef,\n        )\n        total_reward = self.agent.get_buffer_total_reward()\n        solved = total_reward &gt; self.solved_threshold\n        self.agent.onpolicy_reset()\n        print(\n            f\"Episode {episode}, total_reward: {total_reward}, solved: {solved}, loss: {loss}, \"\n            f\"action_entropy: {action_entropy}, advantages_mean: {advantages_mean}\"\n        )\n        if self.config.log_wandb:\n            wandb.log(\n                {\n                    \"episode\": episode,\n                    \"loss\": loss,\n                    \"total_reward\": total_reward,\n                    \"action_entropy\": action_entropy,\n                    \"advantages_mean\": advantages_mean,\n                }\n            )\n\n        if i % self.config.eval_interval == 0:\n            eval_reward = self.evaluate(self.config.eval_episodes)\n            print(f\"Episode {episode}, Eval reward: {eval_reward}\")\n            if self.config.log_wandb:\n                wandb.log({\"eval_reward\": eval_reward, \"episode\": episode})\n</code></pre>"},{"location":"algorithms/a2c/#toyrl.a2c.A2CTrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(num_episodes: int) -&gt; float\n</code></pre> Source code in <code>toyrl/a2c.py</code> <pre><code>def evaluate(self, num_episodes: int) -&gt; float:\n    total_reward = 0.0\n    for _ in range(num_episodes):\n        observation, _ = self.env.reset()\n        terminated, truncated = False, False\n        while not (terminated or truncated):\n            action = self.agent.act(observation, eval=True)\n            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n            observation = next_observation\n            total_reward += float(reward)\n    return total_reward / num_episodes\n</code></pre>"},{"location":"algorithms/dqn/","title":"DQN","text":""},{"location":"algorithms/dqn/#toyrl.dqn.simple_config","title":"toyrl.dqn.simple_config  <code>module-attribute</code>","text":"<pre><code>simple_config = DqnConfig(env_name='CartPole-v1', render_mode=None, solved_threshold=475.0, max_training_steps=500000, learning_rate=0.00025, use_target_network=True, target_soft_update_beta=0.0, target_update_frequency=5, log_wandb=True)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.trainer","title":"toyrl.dqn.trainer  <code>module-attribute</code>","text":"<pre><code>trainer = DqnTrainer(simple_config)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig","title":"toyrl.dqn.DqnConfig  <code>dataclass</code>","text":"<pre><code>DqnConfig(env_name: str = 'CartPole-v1', render_mode: str | None = None, solved_threshold: float = 475.0, gamma: float = 0.999, replay_buffer_capacity: int = 10000, max_training_steps: int = 500000, learning_starts: int = 10000, policy_update_frequency: int = 10, batches_per_training_step: int = 16, batch_size: int = 128, updates_per_batch: int = 1, learning_rate: float = 0.01, use_target_network: bool = False, target_update_frequency: int = 10, target_soft_update_beta: float = 0.0, log_wandb: bool = False)\n</code></pre> <p>Configuration for DQN algorithm.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.env_name","title":"env_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_name: str = 'CartPole-v1'\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.render_mode","title":"render_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>render_mode: str | None = None\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.solved_threshold","title":"solved_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>solved_threshold: float = 475.0\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.999\n</code></pre> <p>The discount factor for future rewards.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.replay_buffer_capacity","title":"replay_buffer_capacity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>replay_buffer_capacity: int = 10000\n</code></pre> <p>The maximum capacity of the experience replay buffer.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.max_training_steps","title":"max_training_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_training_steps: int = 500000\n</code></pre> <p>The maximum number of environment steps to train for.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.learning_starts","title":"learning_starts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_starts: int = 10000\n</code></pre> <p>The number of steps to collect before starting learning.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.policy_update_frequency","title":"policy_update_frequency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>policy_update_frequency: int = 10\n</code></pre> <p>How often to update the policy network (in environment steps).</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.batches_per_training_step","title":"batches_per_training_step  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batches_per_training_step: int = 16\n</code></pre> <p>The number of experience batches to sample in each training step.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 128\n</code></pre> <p>The size of each training batch.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.updates_per_batch","title":"updates_per_batch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>updates_per_batch: int = 1\n</code></pre> <p>The number of optimization steps to perform on each batch.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.01\n</code></pre> <p>The learning rate for the optimizer.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.use_target_network","title":"use_target_network  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_target_network: bool = False\n</code></pre> <p>Whether to use a separate target network (Double DQN when True).</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.target_update_frequency","title":"target_update_frequency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_update_frequency: int = 10\n</code></pre> <p>How often to update the target network (in environment steps).</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.target_soft_update_beta","title":"target_soft_update_beta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_soft_update_beta: float = 0.0\n</code></pre> <p>The soft update parameter for target network (0.0 means hard update).</p>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnConfig.log_wandb","title":"log_wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_wandb: bool = False\n</code></pre> <p>Whether to log the training process to Weights and Biases.</p>"},{"location":"algorithms/dqn/#toyrl.dqn.PolicyNet","title":"toyrl.dqn.PolicyNet","text":"<pre><code>PolicyNet(env_dim: int, action_num: int)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>toyrl/dqn.py</code> <pre><code>def __init__(\n    self,\n    env_dim: int,\n    action_num: int,\n) -&gt; None:\n    super().__init__()\n    self.env_dim = env_dim\n    self.action_num = action_num\n\n    layers = [\n        nn.Linear(self.env_dim, 128),\n        nn.ReLU(),\n        nn.Linear(128, 64),\n        nn.ReLU(),\n        nn.Linear(64, self.action_num),\n    ]\n    self.model = nn.Sequential(*layers)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.PolicyNet.env_dim","title":"env_dim  <code>instance-attribute</code>","text":"<pre><code>env_dim = env_dim\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.PolicyNet.action_num","title":"action_num  <code>instance-attribute</code>","text":"<pre><code>action_num = action_num\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.PolicyNet.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.PolicyNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)  # type: ignore\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience","title":"toyrl.dqn.Experience  <code>dataclass</code>","text":"<pre><code>Experience(terminated: bool, truncated: bool, observation: Any, action: Any, reward: float, next_observation: Any)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.terminated","title":"terminated  <code>instance-attribute</code>","text":"<pre><code>terminated: bool\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.truncated","title":"truncated  <code>instance-attribute</code>","text":"<pre><code>truncated: bool\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.observation","title":"observation  <code>instance-attribute</code>","text":"<pre><code>observation: Any\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Any\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Experience.next_observation","title":"next_observation  <code>instance-attribute</code>","text":"<pre><code>next_observation: Any\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer","title":"toyrl.dqn.ReplayBuffer  <code>dataclass</code>","text":"<pre><code>ReplayBuffer(replay_buffer_size: int = 10000, buffer: list[Experience] = list(), _head_pointer: int = 0)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.replay_buffer_size","title":"replay_buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>replay_buffer_size: int = 10000\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: list[Experience] = field(default_factory=list)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.buffer)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    if len(self.buffer) &lt; self.replay_buffer_size:\n        # Buffer not full yet, append new experience\n        self.buffer.append(experience)\n    else:\n        # Buffer full, overwrite oldest experience\n        index = self._head_pointer % self.replay_buffer_size\n        self.buffer[index] = experience\n\n    # Increment pointer\n    self._head_pointer += 1\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def reset(self) -&gt; None:\n    self.buffer = []\n    self._head_pointer = 0\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.ReplayBuffer.sample","title":"sample","text":"<pre><code>sample(batch_size: int) -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def sample(self, batch_size: int) -&gt; list[Experience]:\n    return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent","title":"toyrl.dqn.Agent","text":"<pre><code>Agent(policy_net: PolicyNet, target_net: PolicyNet | None, optimizer: Optimizer, replay_buffer_size: int)\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def __init__(\n    self,\n    policy_net: PolicyNet,\n    target_net: PolicyNet | None,\n    optimizer: torch.optim.Optimizer,\n    replay_buffer_size: int,\n) -&gt; None:\n    self._policy_net = policy_net\n    self._target_net = target_net\n    self._optimizer = optimizer\n    self._replay_buffer = ReplayBuffer(replay_buffer_size)\n    self._action_num = policy_net.action_num\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self._replay_buffer.add_experience(experience)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent.act","title":"act","text":"<pre><code>act(observation: floating, tau: float) -&gt; tuple[int, float]\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def act(self, observation: np.floating, tau: float) -&gt; tuple[int, float]:\n    x = torch.from_numpy(observation.astype(np.float32))\n    with torch.no_grad():\n        logits = self._policy_net(x)\n    next_action = torch.distributions.Categorical(logits=logits / tau).sample().item()\n    q_value = logits[next_action].item()\n    return next_action, q_value\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent.sample","title":"sample","text":"<pre><code>sample(batch_size: int) -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def sample(self, batch_size: int) -&gt; list[Experience]:\n    return self._replay_buffer.sample(batch_size)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent.policy_update","title":"policy_update","text":"<pre><code>policy_update(gamma: float, experiences: list[Experience]) -&gt; float\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def policy_update(self, gamma: float, experiences: list[Experience]) -&gt; float:\n    observations = torch.tensor([experience.observation for experience in experiences])\n    actions = torch.tensor([experience.action for experience in experiences], dtype=torch.float32)\n    next_observations = torch.tensor([experience.next_observation for experience in experiences])\n    rewards = torch.tensor([experience.reward for experience in experiences])\n    terminated = torch.tensor(\n        [experience.terminated for experience in experiences],\n        dtype=torch.float32,\n    )\n\n    # q preds\n    action_q_preds = self._policy_net(observations).gather(1, actions.long().unsqueeze(1)).squeeze(1)\n\n    with torch.no_grad():\n        next_action_logits = self._policy_net(next_observations)\n        next_actions = torch.argmax(next_action_logits, dim=1)\n        if self._target_net is None:  # Vanilla DQN\n            next_action_q_preds = torch.gather(next_action_logits, 1, next_actions.unsqueeze(1)).squeeze(1)\n        else:  # Double DQN\n            next_action_q_preds = (\n                self._target_net(next_observations).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n            )\n    action_q_targets = rewards + gamma * (1 - terminated) * next_action_q_preds\n    loss = torch.nn.functional.mse_loss(action_q_preds, action_q_targets)\n    # update\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    return loss.item()\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.Agent.polyak_update","title":"polyak_update","text":"<pre><code>polyak_update(beta: float) -&gt; None\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def polyak_update(self, beta: float) -&gt; None:\n    if self._target_net is not None:\n        for target_param, param in zip(self._target_net.parameters(), self._policy_net.parameters()):\n            target_param.data.copy_(beta * target_param.data + (1 - beta) * param.data)\n    else:\n        raise ValueError(\"Target net is not set.\")\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer","title":"toyrl.dqn.DqnTrainer","text":"<pre><code>DqnTrainer(config: DqnConfig)\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def __init__(self, config: DqnConfig) -&gt; None:\n    self.config = config\n    self.env = self._make_env()\n    if isinstance(self.env.action_space, gym.spaces.Discrete) is False:\n        raise ValueError(\"Only discrete action space is supported.\")\n    env_dim = self.env.observation_space.shape[0]  # type: ignore[index]\n    action_num = self.env.action_space.n  # type: ignore[attr-defined]\n\n    policy_net = PolicyNet(env_dim=env_dim, action_num=action_num)\n    optimizer = optim.Adam(policy_net.parameters(), lr=config.learning_rate)\n    if config.use_target_network:\n        target_net = PolicyNet(env_dim=env_dim, action_num=action_num)\n        target_net.load_state_dict(policy_net.state_dict())\n    else:\n        target_net = None\n    self.agent = Agent(\n        policy_net=policy_net,\n        target_net=target_net,\n        optimizer=optimizer,\n        replay_buffer_size=config.replay_buffer_capacity,\n    )\n\n    self.gamma = config.gamma\n    self.solved_threshold = config.solved_threshold\n    if config.log_wandb:\n        wandb.init(\n            # set the wandb project where this run will be logged\n            project=self._get_dqn_name(),\n            name=f\"[{config.env_name}],lr={config.learning_rate}\",\n            # track hyperparameters and run metadata\n            config=asdict(config),\n        )\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = _make_env()\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = Agent(policy_net=policy_net, target_net=target_net, optimizer=optimizer, replay_buffer_size=replay_buffer_capacity)\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.solved_threshold","title":"solved_threshold  <code>instance-attribute</code>","text":"<pre><code>solved_threshold = solved_threshold\n</code></pre>"},{"location":"algorithms/dqn/#toyrl.dqn.DqnTrainer.train","title":"train","text":"<pre><code>train() -&gt; None\n</code></pre> Source code in <code>toyrl/dqn.py</code> <pre><code>def train(self) -&gt; None:\n    tau = 5.0\n    global_step = 0\n    observation, _ = self.env.reset()\n    while global_step &lt; self.config.max_training_steps:\n        global_step += 1\n        # decay tau\n        tau = max(0.1, tau * 0.995)\n\n        action, q_value = self.agent.act(observation, tau)\n        if self.config.log_wandb:\n            wandb.log({\"global_step\": global_step, \"q_value\": q_value})\n\n        next_observation, reward, terminated, truncated, info = self.env.step(action)\n        experience = Experience(\n            observation=observation,\n            action=action,\n            reward=float(reward),\n            next_observation=next_observation,\n            terminated=terminated,\n            truncated=truncated,\n        )\n        self.agent.add_experience(experience)\n        observation = next_observation\n\n        if terminated or truncated:\n            if info and \"episode\" in info:\n                reward = info[\"episode\"][\"r\"]\n                print(f\"global_step={global_step}, episodic_return={reward}\")\n                if self.config.log_wandb:\n                    wandb.log(\n                        {\n                            \"global_step\": global_step,\n                            \"episode_reward\": reward,\n                        }\n                    )\n\n        if self.env.render_mode is not None:\n            self.env.render()\n\n        if global_step &gt;= self.config.learning_starts and global_step % self.config.policy_update_frequency == 0:\n            loss = self._train_step()\n            if self.config.log_wandb:\n                wandb.log(\n                    {\n                        \"global_step\": global_step,\n                        \"loss\": loss,\n                    }\n                )\n        # update target net\n        if self.config.use_target_network and global_step % self.config.target_update_frequency == 0:\n            self.agent.polyak_update(beta=self.config.target_soft_update_beta)\n</code></pre>"},{"location":"algorithms/ppo/","title":"PPO","text":""},{"location":"algorithms/ppo/#toyrl.ppo.default_config","title":"toyrl.ppo.default_config  <code>module-attribute</code>","text":"<pre><code>default_config = PPOConfig(env_name='CartPole-v1', render_mode=None, solved_threshold=475.0, gamma=0.99, lambda_=0.95, epsilon=0.2, entropy_coef=0.01, total_timesteps=1000000, time_horizons=256, update_epochs=4, num_minibatches=4, learning_rate=0.00025, log_wandb=True)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.trainer","title":"toyrl.ppo.trainer  <code>module-attribute</code>","text":"<pre><code>trainer = PPOTrainer(default_config)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig","title":"toyrl.ppo.PPOConfig  <code>dataclass</code>","text":"<pre><code>PPOConfig(env_name: str = 'CartPole-v1', num_envs: int = 4, render_mode: str | None = None, solved_threshold: float = 475.0, gamma: float = 0.999, lambda_: float = 0.98, epsilon: float = 0.2, entropy_coef: float = 0.01, total_timesteps: int = 500000, time_horizons: int = 128, update_epochs: int = 4, num_minibatches: int = 4, learning_rate: float = 0.00025, anneal_learning_rate: bool = True, log_wandb: bool = False)\n</code></pre> <p>Configuration for PPO algorithm.</p>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.env_name","title":"env_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_name: str = 'CartPole-v1'\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.num_envs","title":"num_envs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_envs: int = 4\n</code></pre> <p>The number of parallel game environments</p>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.render_mode","title":"render_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>render_mode: str | None = None\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.solved_threshold","title":"solved_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>solved_threshold: float = 475.0\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.999\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.lambda_","title":"lambda_  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lambda_: float = 0.98\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.epsilon","title":"epsilon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epsilon: float = 0.2\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.entropy_coef","title":"entropy_coef  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entropy_coef: float = 0.01\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.total_timesteps","title":"total_timesteps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_timesteps: int = 500000\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.time_horizons","title":"time_horizons  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_horizons: int = 128\n</code></pre> <p>The number of time steps to collect before updating the policy</p>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.update_epochs","title":"update_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>update_epochs: int = 4\n</code></pre> <p>The K epochs to update the policy</p>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.num_minibatches","title":"num_minibatches  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_minibatches: int = 4\n</code></pre> <p>The number of mini-batches</p>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.00025\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.anneal_learning_rate","title":"anneal_learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>anneal_learning_rate: bool = True\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOConfig.log_wandb","title":"log_wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_wandb: bool = False\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ActorPolicyNet","title":"toyrl.ppo.ActorPolicyNet","text":"<pre><code>ActorPolicyNet(env_dim: int, action_num: int)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>toyrl/ppo.py</code> <pre><code>def __init__(self, env_dim: int, action_num: int) -&gt; None:\n    super().__init__()\n    layers = [\n        nn.Linear(env_dim, 64),\n        nn.Tanh(),\n        nn.Linear(64, 64),\n        nn.Tanh(),\n        nn.Linear(64, action_num),\n    ]\n    self.model = nn.Sequential(*layers)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ActorPolicyNet.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ActorPolicyNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)  # type: ignore\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.CriticValueNet","title":"toyrl.ppo.CriticValueNet","text":"<pre><code>CriticValueNet(env_dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>toyrl/ppo.py</code> <pre><code>def __init__(self, env_dim: int) -&gt; None:\n    super().__init__()\n    layers = [\n        nn.Linear(env_dim, 64),\n        nn.Tanh(),\n        nn.Linear(64, 64),\n        nn.Tanh(),\n        nn.Linear(64, 1),\n    ]\n    self.model = nn.Sequential(*layers)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.CriticValueNet.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.CriticValueNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)  # type: ignore\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience","title":"toyrl.ppo.Experience  <code>dataclass</code>","text":"<pre><code>Experience(env_id: int, terminated: bool, truncated: bool, observation: Any, reward: float, next_observation: Any, action: Any, action_logprob: float, advantage: float | None = None, target_value: float | None = None)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.env_id","title":"env_id  <code>instance-attribute</code>","text":"<pre><code>env_id: int\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.terminated","title":"terminated  <code>instance-attribute</code>","text":"<pre><code>terminated: bool\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.truncated","title":"truncated  <code>instance-attribute</code>","text":"<pre><code>truncated: bool\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.observation","title":"observation  <code>instance-attribute</code>","text":"<pre><code>observation: Any\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.next_observation","title":"next_observation  <code>instance-attribute</code>","text":"<pre><code>next_observation: Any\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Any\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.action_logprob","title":"action_logprob  <code>instance-attribute</code>","text":"<pre><code>action_logprob: float\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.advantage","title":"advantage  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>advantage: float | None = None\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.Experience.target_value","title":"target_value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_value: float | None = None\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer","title":"toyrl.ppo.ReplayBuffer  <code>dataclass</code>","text":"<pre><code>ReplayBuffer(buffer: list[Experience] = list(), env_ids: set[int] = set())\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: list[Experience] = field(default_factory=list)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.env_ids","title":"env_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_ids: set[int] = field(default_factory=set)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.buffer)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self.buffer.append(experience)\n    self.env_ids.add(experience.env_id)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def reset(self) -&gt; None:\n    self.buffer = []\n    self.env_ids = set()\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.ReplayBuffer.sample","title":"sample","text":"<pre><code>sample() -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def sample(self) -&gt; list[Experience]:\n    return self.buffer\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent","title":"toyrl.ppo.PPOAgent","text":"<pre><code>PPOAgent(actor: ActorPolicyNet, critic: CriticValueNet, optimizer: Optimizer)\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def __init__(self, actor: ActorPolicyNet, critic: CriticValueNet, optimizer: optim.Optimizer) -&gt; None:\n    self.actor = actor\n    self.critic = critic\n    self.optimizer = optimizer\n    self._replay_buffer = ReplayBuffer()\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.actor","title":"actor  <code>instance-attribute</code>","text":"<pre><code>actor = actor\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.critic","title":"critic  <code>instance-attribute</code>","text":"<pre><code>critic = critic\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self._replay_buffer.add_experience(experience)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def reset(self) -&gt; None:\n    self._replay_buffer.reset()\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.act","title":"act","text":"<pre><code>act(observation: Any) -&gt; tuple[int, float]\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def act(self, observation: Any) -&gt; tuple[int, float]:\n    x = torch.from_numpy(observation.astype(np.float32))\n    with torch.no_grad():\n        logits = self.actor(x)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    action = torch.distributions.Categorical(probs).sample()\n    action_logprob = torch.log(probs[action])\n    return action.item(), action_logprob.item()\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOAgent.net_update","title":"net_update","text":"<pre><code>net_update(num_minibatches: int, gamma: float, lambda_: float, epsilon: float, entropy_coef: float) -&gt; float\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def net_update(\n    self,\n    num_minibatches: int,\n    gamma: float,\n    lambda_: float,\n    epsilon: float,\n    entropy_coef: float,\n) -&gt; float:\n    raw_experiences = self._replay_buffer.sample()\n    # calculate advantages and target values by GAE\n    experiences = self._calc_adv_v_target(raw_experiences, gamma, lambda_)\n    minibatch_size = len(experiences) // num_minibatches\n    total_loss = 0.0\n    for i in range(num_minibatches):\n        batch_experiences = experiences[minibatch_size * i : minibatch_size * (i + 1)]\n        observations = torch.tensor(np.array([exp.observation for exp in batch_experiences]), dtype=torch.float32)\n        actions = torch.tensor(np.array([exp.action for exp in batch_experiences]), dtype=torch.int64)\n        old_action_logprobs = torch.tensor(\n            np.array([exp.action_logprob for exp in batch_experiences]), dtype=torch.float32\n        )\n        advantages = torch.tensor(np.array([exp.advantage for exp in batch_experiences]), dtype=torch.float32)\n        target_v_values = torch.tensor(\n            np.array([exp.target_value for exp in batch_experiences]), dtype=torch.float32\n        )\n\n        # critic value loss\n        v_values = self.critic(observations).squeeze(1)\n        critic_value_loss = torch.nn.functional.mse_loss(v_values, target_v_values)\n\n        # actor policy loss\n        action_logits = self.actor(observations)\n        action_probs = torch.nn.functional.softmax(action_logits, dim=-1)\n        action_entropy = torch.distributions.Categorical(action_probs).entropy()\n        action_logprobs = torch.gather(action_probs.log(), 1, actions.unsqueeze(1)).squeeze(1)\n        ratios = torch.exp(action_logprobs - old_action_logprobs)\n        surr1 = ratios * advantages\n        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n        actor_policy_loss = -torch.min(surr1, surr2).mean() - entropy_coef * action_entropy.mean()\n\n        loss = actor_policy_loss + critic_value_loss\n        # update actor and critic\n        self.optimizer.zero_grad()\n        loss.backward()\n        # clip\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n        self.optimizer.step()\n\n        total_loss += loss.item()\n    return total_loss / num_minibatches\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOTrainer","title":"toyrl.ppo.PPOTrainer","text":"<pre><code>PPOTrainer(config: PPOConfig)\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def __init__(self, config: PPOConfig) -&gt; None:\n    self.config = config\n    self.envs = self._make_env()\n    env_dim = self.envs.single_observation_space.shape[0]  # type: ignore[index]\n    action_num = self.envs.single_action_space.n  # type: ignore[attr-defined]\n    actor = ActorPolicyNet(env_dim=env_dim, action_num=action_num)\n    critic = CriticValueNet(env_dim=env_dim)\n    optimizer = torch.optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=config.learning_rate)\n    self.agent = PPOAgent(actor=actor, critic=critic, optimizer=optimizer)\n    if config.log_wandb:\n        wandb.init(\n            # set the wandb project where this run will be logged\n            project=\"PPO\",\n            name=f\"[{config.env_name}]lr={config.learning_rate}\",\n            # track hyperparameters and run metadata\n            config=asdict(config),\n        )\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOTrainer.envs","title":"envs  <code>instance-attribute</code>","text":"<pre><code>envs = _make_env()\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOTrainer.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = PPOAgent(actor=actor, critic=critic, optimizer=optimizer)\n</code></pre>"},{"location":"algorithms/ppo/#toyrl.ppo.PPOTrainer.train","title":"train","text":"<pre><code>train()\n</code></pre> Source code in <code>toyrl/ppo.py</code> <pre><code>def train(self):\n    batch_size = self.config.time_horizons * self.config.num_envs\n    num_iteration = self.config.total_timesteps // batch_size\n\n    global_step = 0\n    observations, _ = self.envs.reset()\n    for iteration in range(num_iteration):\n        if self.config.anneal_learning_rate:\n            frac = 1.0 - iteration / num_iteration\n            lr = frac * self.config.learning_rate\n            self.agent.optimizer.param_groups[0][\"lr\"] = lr\n\n        # Collect experience\n        for step in range(self.config.time_horizons):\n            global_step += self.config.num_envs\n            actions, action_logprobs = [], []\n            for obs in observations:\n                action, action_logprob = self.agent.act(obs)\n                actions.append(action)\n                action_logprobs.append(action_logprob)\n            next_observations, rewards, terminateds, truncateds, infos = self.envs.step(np.array(actions))\n            for env_id in range(self.config.num_envs):\n                experience = Experience(\n                    env_id=env_id,\n                    terminated=terminateds[env_id],\n                    truncated=truncateds[env_id],\n                    observation=observations[env_id],\n                    action=actions[env_id],\n                    action_logprob=action_logprobs[env_id],\n                    reward=float(rewards[env_id]),\n                    next_observation=next_observations[env_id],\n                )\n                self.agent.add_experience(experience)\n            observations = next_observations\n\n            if \"episode\" in infos:\n                for i in range(self.config.num_envs):\n                    if infos[\"_episode\"][i]:\n                        print(f\"global_step={global_step}, episodic_return={infos['episode']['r'][i]}\")\n                        if self.config.log_wandb:\n                            wandb.log(\n                                {\n                                    \"global_step\": global_step,\n                                    \"episodic_return\": infos[\"episode\"][\"r\"][i],\n                                }\n                            )\n\n        # Update policy\n        total_loss = 0.0\n        for _ in range(self.config.update_epochs):\n            loss = self.agent.net_update(\n                gamma=self.config.gamma,\n                lambda_=self.config.lambda_,\n                epsilon=self.config.epsilon,\n                entropy_coef=self.config.entropy_coef,\n                num_minibatches=self.config.num_minibatches,\n            )\n            total_loss += loss\n        loss = total_loss / self.config.update_epochs\n        if self.config.log_wandb:\n            wandb.log(\n                {\n                    \"global_step\": global_step,\n                    \"learning_rate\": self.agent.optimizer.param_groups[0][\"lr\"],\n                    \"loss\": loss,\n                }\n            )\n        # Onpolicy reset\n        self.agent.reset()\n</code></pre>"},{"location":"algorithms/reinforce/","title":"REINFORCE","text":""},{"location":"algorithms/reinforce/#toyrl.reinforce.default_config","title":"toyrl.reinforce.default_config  <code>module-attribute</code>","text":"<pre><code>default_config = ReinforceConfig(env_name='CartPole-v1', render_mode=None, solved_threshold=475.0, num_episodes=100000, learning_rate=0.002, with_baseline=True, log_wandb=True)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.trainer","title":"toyrl.reinforce.trainer  <code>module-attribute</code>","text":"<pre><code>trainer = ReinforceTrainer(default_config)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig","title":"toyrl.reinforce.ReinforceConfig  <code>dataclass</code>","text":"<pre><code>ReinforceConfig(env_name: str = 'CartPole-v1', render_mode: str | None = None, solved_threshold: float = 475.0, gamma: float = 0.999, num_episodes: int = 500, learning_rate: float = 0.01, with_baseline: bool = True, log_wandb: bool = False)\n</code></pre> <p>Configuration for REINFORCE algorithm.</p>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.env_name","title":"env_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_name: str = 'CartPole-v1'\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.render_mode","title":"render_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>render_mode: str | None = None\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.solved_threshold","title":"solved_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>solved_threshold: float = 475.0\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.999\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.num_episodes","title":"num_episodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_episodes: int = 500\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.01\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.with_baseline","title":"with_baseline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_baseline: bool = True\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceConfig.log_wandb","title":"log_wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_wandb: bool = False\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.PolicyNet","title":"toyrl.reinforce.PolicyNet","text":"<pre><code>PolicyNet(env_dim: int, action_num: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A simple policy network for REINFORCE.</p> Source code in <code>toyrl/reinforce.py</code> <pre><code>def __init__(self, env_dim: int, action_num: int) -&gt; None:\n    super().__init__()\n    layers = [\n        nn.Linear(env_dim, 64),\n        nn.ReLU(),\n        nn.Linear(64, action_num),\n    ]\n    self.model = nn.Sequential(*layers)\n    self.train()\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.PolicyNet.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.PolicyNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)  # type: ignore\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience","title":"toyrl.reinforce.Experience  <code>dataclass</code>","text":"<pre><code>Experience(observation: Any, action: Any, action_log_prob: Tensor, reward: float, next_observation: Any, terminated: bool, truncated: bool)\n</code></pre> <p>An experience for REINFORCE.</p>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.observation","title":"observation  <code>instance-attribute</code>","text":"<pre><code>observation: Any\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Any\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.action_log_prob","title":"action_log_prob  <code>instance-attribute</code>","text":"<pre><code>action_log_prob: Tensor\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.next_observation","title":"next_observation  <code>instance-attribute</code>","text":"<pre><code>next_observation: Any\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.terminated","title":"terminated  <code>instance-attribute</code>","text":"<pre><code>terminated: bool\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Experience.truncated","title":"truncated  <code>instance-attribute</code>","text":"<pre><code>truncated: bool\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer","title":"toyrl.reinforce.ReplayBuffer  <code>dataclass</code>","text":"<pre><code>ReplayBuffer(buffer: list[Experience] = list())\n</code></pre> <p>A replay buffer for REINFORCE.</p>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: list[Experience] = field(default_factory=list)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.buffer)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self.buffer.append(experience)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def reset(self) -&gt; None:\n    self.buffer = []\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.sample","title":"sample","text":"<pre><code>sample() -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def sample(self) -&gt; list[Experience]:\n    return self.buffer\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReplayBuffer.total_reward","title":"total_reward","text":"<pre><code>total_reward() -&gt; float\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def total_reward(self) -&gt; float:\n    return sum(experience.reward for experience in self.buffer)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent","title":"toyrl.reinforce.Agent","text":"<pre><code>Agent(policy_net: Module, optimizer: Optimizer)\n</code></pre> <p>An agent for REINFORCE.</p> Source code in <code>toyrl/reinforce.py</code> <pre><code>def __init__(self, policy_net: nn.Module, optimizer: torch.optim.Optimizer) -&gt; None:\n    self._policy_net = policy_net\n    self._optimizer = optimizer\n    self._replay_buffer = ReplayBuffer()\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent.onpolicy_reset","title":"onpolicy_reset","text":"<pre><code>onpolicy_reset() -&gt; None\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def onpolicy_reset(self) -&gt; None:\n    self._replay_buffer.reset()\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self._replay_buffer.add_experience(experience)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent.get_buffer_total_reward","title":"get_buffer_total_reward","text":"<pre><code>get_buffer_total_reward() -&gt; float\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def get_buffer_total_reward(self) -&gt; float:\n    return self._replay_buffer.total_reward()\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent.act","title":"act","text":"<pre><code>act(observation: floating) -&gt; tuple[int, Tensor]\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def act(self, observation: np.floating) -&gt; tuple[int, torch.Tensor]:\n    x = torch.from_numpy(observation.astype(np.float32))\n    logits = self._policy_net(x)\n    next_action_dist = torch.distributions.Categorical(logits=logits)\n    action = next_action_dist.sample()\n    action_log_prob = next_action_dist.log_prob(action)\n    return action.item(), action_log_prob\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.Agent.policy_update","title":"policy_update","text":"<pre><code>policy_update(gamma: float, with_baseline: bool) -&gt; float\n</code></pre> Source code in <code>toyrl/reinforce.py</code> <pre><code>def policy_update(self, gamma: float, with_baseline: bool) -&gt; float:\n    experiences = self._replay_buffer.sample()\n    # returns\n    T = len(experiences)\n    returns = torch.zeros(T)\n    future_ret = 0.0\n    for t in reversed(range(T)):\n        future_ret = experiences[t].reward + gamma * future_ret\n        returns[t] = future_ret\n    # baseline\n    if with_baseline:\n        returns -= returns.mean()\n\n    # log_probs\n    action_log_probs = [exp.action_log_prob for exp in experiences]\n    log_probs = torch.stack(action_log_probs)\n    # loss\n    loss = -log_probs * returns\n    loss = torch.sum(loss)\n    # update\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    return loss.item()\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer","title":"toyrl.reinforce.ReinforceTrainer","text":"<pre><code>ReinforceTrainer(config: ReinforceConfig)\n</code></pre> <p>A trainer for REINFORCE.</p> Source code in <code>toyrl/reinforce.py</code> <pre><code>def __init__(self, config: ReinforceConfig) -&gt; None:\n    self.config = config\n    self.env = gym.make(config.env_name, render_mode=config.render_mode)\n    env_dim = self.env.observation_space.shape[0]  # type: ignore[index]\n    action_num = self.env.action_space.n  # type: ignore[attr-defined]\n    policy_net = PolicyNet(env_dim=env_dim, action_num=action_num)\n    optimizer = optim.Adam(policy_net.parameters(), lr=config.learning_rate)\n    self.agent = Agent(policy_net=policy_net, optimizer=optimizer)\n\n    self.num_episodes = config.num_episodes\n    self.gamma = config.gamma\n    self.with_baseline = config.with_baseline\n    self.solved_threshold = config.solved_threshold\n    if config.log_wandb:\n        wandb.init(\n            # set the wandb project where this run will be logged\n            project=\"Reinforce\",\n            name=f\"[{config.env_name}]lr={config.learning_rate}, baseline={config.with_baseline}\",\n            # track hyperparameters and run metadata\n            config=asdict(config),\n        )\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = make(env_name, render_mode=render_mode)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = Agent(policy_net=policy_net, optimizer=optimizer)\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.num_episodes","title":"num_episodes  <code>instance-attribute</code>","text":"<pre><code>num_episodes = num_episodes\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.with_baseline","title":"with_baseline  <code>instance-attribute</code>","text":"<pre><code>with_baseline = with_baseline\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.solved_threshold","title":"solved_threshold  <code>instance-attribute</code>","text":"<pre><code>solved_threshold = solved_threshold\n</code></pre>"},{"location":"algorithms/reinforce/#toyrl.reinforce.ReinforceTrainer.train","title":"train","text":"<pre><code>train() -&gt; None\n</code></pre> <p>Train the agent.</p> Source code in <code>toyrl/reinforce.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Train the agent.\"\"\"\n    for epi in range(self.num_episodes):\n        observation, _ = self.env.reset()\n        terminated, truncated = False, False\n        while not (terminated or truncated):\n            action, action_log_prob = self.agent.act(observation)\n            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n            experience = Experience(\n                observation=observation,\n                action=action,\n                action_log_prob=action_log_prob,\n                reward=float(reward),\n                terminated=terminated,\n                truncated=truncated,\n                next_observation=next_observation,\n            )\n            self.agent.add_experience(experience)\n            observation = next_observation\n            if self.config.render_mode is not None:\n                self.env.render()\n        loss = self.agent.policy_update(\n            gamma=self.gamma,\n            with_baseline=self.with_baseline,\n        )\n        total_reward = self.agent.get_buffer_total_reward()\n        solved = total_reward &gt; self.solved_threshold\n        self.agent.onpolicy_reset()\n        print(f\"Episode {epi}, loss: {loss}, total_reward: {total_reward}, solved: {solved}\")\n        if self.config.log_wandb:\n            wandb.log(\n                {\n                    \"episode\": epi,\n                    \"loss\": loss,\n                    \"total_reward\": total_reward,\n                }\n            )\n</code></pre>"},{"location":"algorithms/sarsa/","title":"SARSA","text":""},{"location":"algorithms/sarsa/#toyrl.sarsa.default_config","title":"toyrl.sarsa.default_config  <code>module-attribute</code>","text":"<pre><code>default_config = SarsaConfig(env_name='CartPole-v1', render_mode=None, solved_threshold=475.0, max_training_steps=2000000, learning_rate=0.01, log_wandb=True)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.trainer","title":"toyrl.sarsa.trainer  <code>module-attribute</code>","text":"<pre><code>trainer = SarsaTrainer(default_config)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig","title":"toyrl.sarsa.SarsaConfig  <code>dataclass</code>","text":"<pre><code>SarsaConfig(env_name: str = 'CartPole-v1', render_mode: str | None = None, solved_threshold: float = 475.0, gamma: float = 0.999, max_training_steps: int = 500000, learning_rate: float = 0.00025, log_wandb: bool = False)\n</code></pre> <p>Configuration for SARSA algorithm.</p>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.env_name","title":"env_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>env_name: str = 'CartPole-v1'\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.render_mode","title":"render_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>render_mode: str | None = None\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.solved_threshold","title":"solved_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>solved_threshold: float = 475.0\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.999\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.max_training_steps","title":"max_training_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_training_steps: int = 500000\n</code></pre> <p>The maximum number of environment steps to train for.</p>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.00025\n</code></pre> <p>The learning rate for the optimizer.</p>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaConfig.log_wandb","title":"log_wandb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_wandb: bool = False\n</code></pre> <p>Whether to log the training process to Weights and Biases.</p>"},{"location":"algorithms/sarsa/#toyrl.sarsa.PolicyNet","title":"toyrl.sarsa.PolicyNet","text":"<pre><code>PolicyNet(env_dim: int, action_num: int)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>toyrl/sarsa.py</code> <pre><code>def __init__(\n    self,\n    env_dim: int,\n    action_num: int,\n) -&gt; None:\n    super().__init__()\n    self.env_dim = env_dim\n    self.action_num = action_num\n\n    layers = [\n        nn.Linear(self.env_dim, 128),\n        nn.ReLU(),\n        nn.Linear(128, self.action_num),\n    ]\n    self.model = nn.Sequential(*layers)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.PolicyNet.env_dim","title":"env_dim  <code>instance-attribute</code>","text":"<pre><code>env_dim = env_dim\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.PolicyNet.action_num","title":"action_num  <code>instance-attribute</code>","text":"<pre><code>action_num = action_num\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.PolicyNet.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.PolicyNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)  # type: ignore\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience","title":"toyrl.sarsa.Experience  <code>dataclass</code>","text":"<pre><code>Experience(terminated: bool, truncated: bool, observation: Any, action: Any, reward: float, next_observation: Any = None, next_action: Any = None)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.terminated","title":"terminated  <code>instance-attribute</code>","text":"<pre><code>terminated: bool\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.truncated","title":"truncated  <code>instance-attribute</code>","text":"<pre><code>truncated: bool\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.observation","title":"observation  <code>instance-attribute</code>","text":"<pre><code>observation: Any\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Any\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.next_observation","title":"next_observation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>next_observation: Any = None\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Experience.next_action","title":"next_action  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>next_action: Any = None\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer","title":"toyrl.sarsa.ReplayBuffer  <code>dataclass</code>","text":"<pre><code>ReplayBuffer(buffer: list[Experience] = list())\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer.buffer","title":"buffer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>buffer: list[Experience] = field(default_factory=list)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.buffer)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self.buffer.append(experience)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def reset(self) -&gt; None:\n    self.buffer = []\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.ReplayBuffer.sample","title":"sample","text":"<pre><code>sample(with_next_sa: bool = True) -&gt; list[Experience]\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def sample(self, with_next_sa: bool = True) -&gt; list[Experience]:\n    if with_next_sa is False:\n        return self.buffer\n    else:\n        res = []\n        for i in range(len(self.buffer) - 1):\n            experience = self.buffer[i]\n            next_experience = self.buffer[i + 1]\n            res.append(\n                Experience(\n                    observation=experience.observation,\n                    action=experience.action,\n                    reward=experience.reward,\n                    next_observation=next_experience.observation,\n                    next_action=next_experience.action,\n                    terminated=next_experience.terminated,\n                    truncated=next_experience.truncated,\n                )\n            )\n        return res\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Agent","title":"toyrl.sarsa.Agent","text":"<pre><code>Agent(policy_net: PolicyNet, optimizer: Optimizer)\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def __init__(self, policy_net: PolicyNet, optimizer: torch.optim.Optimizer) -&gt; None:\n    self._policy_net = policy_net\n    self._optimizer = optimizer\n    self._replay_buffer = ReplayBuffer()\n    self._action_num = policy_net.action_num\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Agent.onpolicy_reset","title":"onpolicy_reset","text":"<pre><code>onpolicy_reset() -&gt; None\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def onpolicy_reset(self) -&gt; None:\n    self._replay_buffer.reset()\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Agent.add_experience","title":"add_experience","text":"<pre><code>add_experience(experience: Experience) -&gt; None\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def add_experience(self, experience: Experience) -&gt; None:\n    self._replay_buffer.add_experience(experience)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Agent.act","title":"act","text":"<pre><code>act(observation: floating, epsilon: float) -&gt; int\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def act(self, observation: np.floating, epsilon: float) -&gt; int:\n    if np.random.rand() &lt; epsilon:\n        action = np.random.randint(self._action_num)\n        return action\n    x = torch.from_numpy(observation.astype(np.float32))\n    with torch.no_grad():\n        logits = self._policy_net(x)\n    action = int(torch.argmax(logits).item())\n    return action\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.Agent.policy_update","title":"policy_update","text":"<pre><code>policy_update(gamma: float) -&gt; float\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def policy_update(self, gamma: float) -&gt; float:\n    experiences = self._replay_buffer.sample()\n\n    observations = torch.tensor([experience.observation for experience in experiences])\n    actions = torch.tensor([experience.action for experience in experiences], dtype=torch.float32)\n    next_observations = torch.tensor([experience.next_observation for experience in experiences])\n    next_actions = torch.tensor([experience.next_action for experience in experiences])\n    rewards = torch.tensor([experience.reward for experience in experiences])\n    terminated = torch.tensor(\n        [experience.terminated for experience in experiences],\n        dtype=torch.float32,\n    )\n\n    # q preds\n    action_q_preds = self._policy_net(observations).gather(1, actions.long().unsqueeze(1)).squeeze(1)\n    with torch.no_grad():\n        next_action_q_preds = self._policy_net(next_observations).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n        q_targets = rewards + gamma * (1 - terminated) * next_action_q_preds\n    loss = torch.nn.functional.mse_loss(action_q_preds, q_targets)\n    # update\n    self._optimizer.zero_grad()\n    loss.backward()\n    # clip grad\n    torch.nn.utils.clip_grad_norm_(self._policy_net.parameters(), max_norm=1.0)\n    self._optimizer.step()\n    return loss.item()\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer","title":"toyrl.sarsa.SarsaTrainer","text":"<pre><code>SarsaTrainer(config: SarsaConfig)\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def __init__(self, config: SarsaConfig) -&gt; None:\n    self.config = config\n    self.env = self._make_env()\n    if isinstance(self.env.action_space, gym.spaces.Discrete) is False:\n        raise ValueError(\"Only discrete action space is supported.\")\n    env_dim = self.env.observation_space.shape[0]  # type: ignore[index]\n    action_num = self.env.action_space.n  # type: ignore[attr-defined]\n    policy_net = PolicyNet(env_dim=env_dim, action_num=action_num)\n    optimizer = optim.Adam(policy_net.parameters(), lr=config.learning_rate)\n    self.agent = Agent(policy_net=policy_net, optimizer=optimizer)\n\n    self.gamma = config.gamma\n    self.solved_threshold = config.solved_threshold\n    if config.log_wandb:\n        wandb.init(\n            # set the wandb project where this run will be logged\n            project=\"SARSA\",\n            name=f\"[{config.env_name}],lr={config.learning_rate}\",\n            # track hyperparameters and run metadata\n            config=asdict(config),\n        )\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env = _make_env()\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.agent","title":"agent  <code>instance-attribute</code>","text":"<pre><code>agent = Agent(policy_net=policy_net, optimizer=optimizer)\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.solved_threshold","title":"solved_threshold  <code>instance-attribute</code>","text":"<pre><code>solved_threshold = solved_threshold\n</code></pre>"},{"location":"algorithms/sarsa/#toyrl.sarsa.SarsaTrainer.train","title":"train","text":"<pre><code>train() -&gt; None\n</code></pre> Source code in <code>toyrl/sarsa.py</code> <pre><code>def train(self) -&gt; None:\n    epsilon = 1.0\n    global_step = 0\n\n    observation, _ = self.env.reset()\n    while global_step &lt; self.config.max_training_steps:\n        global_step += 1\n        epsilon = max(0.05, epsilon * 0.9999)\n\n        action = self.agent.act(observation, epsilon)\n        next_observation, reward, terminated, truncated, info = self.env.step(action)\n        experience = Experience(\n            observation=observation,\n            action=action,\n            reward=float(reward),\n            next_observation=next_observation,\n            terminated=terminated,\n            truncated=truncated,\n        )\n        self.agent.add_experience(experience)\n        observation = next_observation\n        if self.env.render_mode is not None:\n            self.env.render()\n\n        if terminated or truncated:\n            if info and \"episode\" in info:\n                episode_reward = info[\"episode\"][\"r\"]\n                loss = self.agent.policy_update(gamma=self.gamma)\n                self.agent.onpolicy_reset()\n                print(\n                    f\"global_step={global_step}, epsilon={epsilon}, episodic_return={episode_reward}, loss={loss}\"\n                )\n                if self.config.log_wandb:\n                    wandb.log(\n                        {\n                            \"global_step\": global_step,\n                            \"episode_reward\": episode_reward,\n                            \"loss\": loss,\n                        }\n                    )\n</code></pre>"}]}